---
layout: default
title: rPICKLE Method
math: true
---

# Randomized Physics-informed Conditional Karhunen-Loève expansions (rPICKLE) for Large-scale High-dimensional Bayesian Inversion

Implementations of the "randomize-then-optimize" approach for sampling PICKLE posteriors in the Bayesian framework, as described in the paper:

> **Randomized physics-informed machine learning for uncertainty quantification in high-dimensional inverse problems**  
> *Journal of Computational Physics, 2024*  
> [[https://doi.org/10.1016/j.cma.2024.117670](https://doi.org/10.1016/j.cma.2024.117670)](https://doi.org/10.1016/j.jcp.2024.113395)

---
## Overview
We propose the randomized physics-informed conditional Karhunen-Loève expansion (rPICKLE) method for uncertainty quantification in high-dimensional inverse problems. In rPICKLE, the states and parameters of the governing partial differential equation (PDE) are approximated via truncated conditional Karhunen-Loève expansions (cKLEs). Uncertainty in the inverse solution is quantified via the posterior distribution of cKLE coefficients formulated with independent standard normal priors and a likelihood containing PDE residuals evaluated over the computational domain. This method provides a scalable alternative to traditional Bayesian inference techniques for physics-constrained problems.

## Mathematics in briefs
Consider solving a BVP in the form
$$
\mathcal{L}(u(\mathbf{x}),y(\mathbf{x}))=0, \quad \mathbf{x} \in \Omega
$$
We approximate PDE parameter $y$ and state $u$ as truncated KLEs:
$$
u(x,t) = \hat{u}(x,t ; \boldsymbol\eta) & \approx \overline{u}(x,t) + \sum_{i=1}^{N_{\eta}} \phi_u^i(x,t) \sqrt{\lambda_u^i} \eta^i
$$
and 
$$
y(\mathbf{x}) \approx \overline{y}(\mathbf{x}) + \sum_{i=1}^{N_{\xi}} \phi_y^{i}(\mathbf{x}) \sqrt{\lambda_y^{i}} \xi^i,
$$
In this way, we reduce the dimension of the parameter and state to $N_{\xi}$ and $N_{\eta}$, respectively. These values are chosen to retain specified energy/information of the field. Then, we formulate the deterministic PICKLE loss function as described in [Yeung et al. (2022)](https://doi.org/10.1029/2021WR031023).
$$
(\boldsymbol\xi^*, \boldsymbol\eta^*) 
 =  \arg\min_{\boldsymbol\xi, \boldsymbol\eta} \frac{1}{2}\|\mathcal{R}(\boldsymbol\xi, \boldsymbol\eta)\|^2_2 +  \frac{\gamma}{2}\|\boldsymbol\xi \|_2^2 + \frac{\gamma}{2}\|\boldsymbol\eta \|_2^2,
$$
The optimal solution to this problem is a MAP estimation, but in the latent KLE space. A major advantage is the computational cost of PICKLE increases near linearly (as $N^1.15$) with the number of grid nodes N, while that of MAP in the original dimension increases as cubicly ($N^3.28$). ![Alt text for the image](path/to/image.png)

In rPICKLE, the PICKLE loss function is randomized as
\begin{eqnarray}\label{eq:rPICKLE_loss}
 L^r(\boldsymbol\xi, \boldsymbol\eta; \boldsymbol\omega, \boldsymbol\alpha, \boldsymbol\beta)
& = &
 \frac{1}{2}\|\mathcal{R}(\boldsymbol\xi, \boldsymbol\eta) - \boldsymbol\omega\|_{\Sigma_r}^2 +  \frac{1}{2}\| \boldsymbol\xi -\boldsymbol\alpha\|_{\Sigma_\xi}^2 \\ \nonumber
& + &\frac{1}{2}\| \boldsymbol\eta  - \boldsymbol\beta\|_{\Sigma_\eta}^2,
\end{eqnarray}
where $\boldsymbol\omega$, $\boldsymbol\alpha$, and $\boldsymbol\beta$ are independent zero-mean random noise vectors with covariances $\Sigma_w = \Sigma_r = {\sigma_r^2}\mathbf{I}$, $\Sigma_\alpha = \Sigma_\xi = \mathbf{I}$, and $\Sigma_\beta = \Sigma_\eta = \mathbf{I}$, respectively. Then, the samples of the posterior distribution $P(\boldsymbol\xi, \boldsymbol\eta | \mathcal{D}_{r})$ are generated by repeatedly solving the minimization problem.
This is like sampling a Bayesian posterior
$$
P(\boldsymbol\xi, \boldsymbol\eta | \mathcal{D}_{r}) = \frac{P(\mathcal{D}_{r} | \boldsymbol\xi, \boldsymbol\eta)P(\boldsymbol\xi, \boldsymbol\eta)}
{\int\int P(\mathcal{D}_{r} | \boldsymbol\xi, \boldsymbol\eta )P(\boldsymbol\xi, \boldsymbol\eta ) d\boldsymbol\xi d\boldsymbol\eta},
$$
where the likelihood is 
$$
 P(\mathcal{D}_{r} | \boldsymbol\xi, \boldsymbol\eta) = \left ( \frac{1}{\sqrt{2\pi}\sigma_{r}} \right )^N \exp \left (-\frac{1}{2} \mathcal{R}(\boldsymbol\xi, \boldsymbol\eta)^T\Sigma_r^{-1} \mathcal{R}(\boldsymbol\xi, \boldsymbol\eta) \right )
$$

If you find this code useful for your research, please cite the following paper:
```bibtex
@article{zong2024randomized,
  title={Randomized physics-informed machine learning for uncertainty quantification in high-dimensional inverse problems},
  author={Zong, Yifei and Barajas-Solano, David and Tartakovsky, Alexandre M},
  journal={Journal of Computational Physics},
  volume={519},
  pages={113395},
  year={2024},
  publisher={Elsevier}
}
```
